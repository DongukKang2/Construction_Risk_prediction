{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "#transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#장고에서는 어떻게 처리가능한 부분인지 검토 필요!\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파이토치 클래스 정의\n",
    "class BERTClassifier1(nn.Module):\n",
    "    def __init__(self, bert, hidden_size=768, num_classes=1, dr_rate=None, params=None):\n",
    "        super(BERTClassifier1, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "\n",
    "        self.classifier1 = nn.Linear(hidden_size, num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout1 = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "\n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout1(pooler)\n",
    "        return self.classifier1(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier2(nn.Module):\n",
    "    def __init__(self, bert, hidden_size=768, num_classes=3, dr_rate=None, params=None):\n",
    "        super(BERTClassifier2, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "\n",
    "        self.classifier2 = nn.Linear(hidden_size, num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout2 = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "\n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout2(pooler)\n",
    "        return self.classifier2(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier3(nn.Module):\n",
    "    def __init__(self, bert, hidden_size=768, num_classes=3, dr_rate=None, params=None):\n",
    "        super(BERTClassifier3, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "\n",
    "        self.classifier3 = nn.Linear(hidden_size, num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout3 = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "\n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout3(pooler)\n",
    "        return self.classifier3(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier4(nn.Module):\n",
    "    def __init__(self, bert, hidden_size=768, num_classes=10, dr_rate=None, params=None):\n",
    "        super(BERTClassifier4, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "\n",
    "        self.classifier4 = nn.Linear(hidden_size, num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout4 = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "\n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout4(pooler)\n",
    "        return self.classifier4(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습모델 불러오기.\n",
    "model1 = BERTClassifier1(bertmodel, dr_rate=0.5).to(device)\n",
    "model2 = BERTClassifier2(bertmodel, dr_rate=0.5).to(device)\n",
    "model3 = BERTClassifier3(bertmodel, dr_rate=0.5).to(device)\n",
    "model4 = BERTClassifier4(bertmodel, dr_rate=0.5).to(device)\n",
    "\n",
    "checkpoint=torch.load('wbs_classifier_ver101.pth') #pth 파일 경로 설정\n",
    "model1.load_state_dict(checkpoint['model1_state_dict'])\n",
    "model2.load_state_dict(checkpoint['model2_state_dict'])\n",
    "model3.load_state_dict(checkpoint['model3_state_dict'])\n",
    "model4.load_state_dict(checkpoint['model4_state_dict'])\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "model3.eval()\n",
    "model4.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터셋 클래스 정의\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "        \n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter 정의\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "max_len = 64              \n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#예측 모델 함수 정의\n",
    "def predictlv3(predict_sentence):\n",
    "    \n",
    "    data1 = [predict_sentence, '0']\n",
    "    dataset_another1 = [data1]\n",
    "\n",
    "    another_test1 = BERTDataset(dataset_another1, 0, 1, tok, max_len, True, False)\n",
    "    test_dataloader1 = torch.utils.data.DataLoader(another_test1, batch_size=batch_size, num_workers=5)\n",
    "\n",
    "    model1.eval()\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader1):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model1(token_ids, valid_length, segment_ids)\n",
    "\n",
    "\n",
    "        total=[]\n",
    "        for i in out:\n",
    "            logits=i\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            if np.argmax(logits) == 0:\n",
    "                total.append(\"토공\") #학습된 WBS 내용에 따라 목록 추가\n",
    "        \n",
    "                \n",
    "        return total[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictlv4(predict_sentence):\n",
    "    \n",
    "    data2 = [predict_sentence, '0']\n",
    "    dataset_another2 = [data2]\n",
    "\n",
    "    another_test2 = BERTDataset(dataset_another2, 0, 1, tok, max_len, True, False)\n",
    "    test_dataloader2 = torch.utils.data.DataLoader(another_test2, batch_size=batch_size, num_workers=5)\n",
    "\n",
    "    model2.eval()\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader2):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model2(token_ids, valid_length, segment_ids)\n",
    "\n",
    "\n",
    "        total=[]\n",
    "        for i in out:\n",
    "            logits=i\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            if np.argmax(logits) == 0: #학습된 WBS 내용에 따라 목록 추가\n",
    "                total.append(\"본선\")\n",
    "            elif np.argmax(logits) == 1:\n",
    "                total.append(\"IC/JC\")\n",
    "            elif np.argmax(logits) == 2:\n",
    "                total.append(\"지선/부체도로\")\n",
    "            \n",
    "                       \n",
    "        return total[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictlv7_1(predict_sentence):\n",
    "    \n",
    "    data3 = [predict_sentence, '0']\n",
    "    dataset_another3 = [data3]\n",
    "\n",
    "    another_test3 = BERTDataset(dataset_another3, 0, 1, tok, max_len, True, False)\n",
    "    test_dataloader3 = torch.utils.data.DataLoader(another_test3, batch_size=batch_size, num_workers=5)\n",
    "\n",
    "    model3.eval()\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader3):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model3(token_ids, valid_length, segment_ids)\n",
    "\n",
    "\n",
    "        total=[]\n",
    "        for i in out:\n",
    "            logits=i\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            if np.argmax(logits) == 0: #학습된 WBS 내용에 따라 목록 추가\n",
    "                total.append(\"흙깍기\")\n",
    "            elif np.argmax(logits) == 1:\n",
    "                total.append(\"흙쌓기\")\n",
    "            elif np.argmax(logits) == 2:\n",
    "                total.append(\"토공기타\")\n",
    "               \n",
    "            \n",
    "            \n",
    "                \n",
    "        return total[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictlv7_2(predict_sentence):\n",
    "    \n",
    "    data4 = [predict_sentence, '0']\n",
    "    dataset_another4 = [data4]\n",
    "\n",
    "    another_test4 = BERTDataset(dataset_another4, 0, 1, tok, max_len, True, False)\n",
    "    test_dataloader4 = torch.utils.data.DataLoader(another_test4, batch_size=batch_size, num_workers=5)\n",
    "\n",
    "    model4.eval()\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader4):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model4(token_ids, valid_length, segment_ids)\n",
    "\n",
    "\n",
    "        total=[]\n",
    "        for i in out:\n",
    "            logits=i\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            if np.argmax(logits) == 0: #학습된 WBS 내용에 따라 목록 추가\n",
    "                total.append(\"노상\")\n",
    "            elif np.argmax(logits) == 1:\n",
    "                total.append(\"노체\")\n",
    "            elif np.argmax(logits) == 2:\n",
    "                total.append(\"리핑\")\n",
    "            elif np.argmax(logits) == 3:\n",
    "                total.append(\"발파\")\n",
    "            elif np.argmax(logits) == 4:\n",
    "                total.append(\"비탈면보호공\")\n",
    "            elif np.argmax(logits) == 5:\n",
    "                total.append(\"연약지반처리\")\n",
    "            elif np.argmax(logits) == 6:\n",
    "                total.append(\"옹벽기타\")\n",
    "            elif np.argmax(logits) == 7:\n",
    "                total.append(\"토공기타\")\n",
    "            elif np.argmax(logits) == 8:\n",
    "                total.append(\"흙깍기기타\")\n",
    "            elif np.argmax(logits) == 9:\n",
    "                total.append(\"흙쌓기기타\")     \n",
    "           \n",
    "            \n",
    "                \n",
    "        return total[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#사전 정의된 분석 문서 불러오기\n",
    "input_data = ['RAMP-E 0+660 ~ 0+720 암 성토(B)', 'RAMP-E 0+600 ~ 0+800 2공구 반입토 운반 및 성토(C)', '부체도로 35;2 : 임목폐기물 집목[C]']\n",
    "#웹서비스에서 각각 입력된 작업일지의 내용을 가져옴. ex. ['text1', 'text2', 'text3']\n",
    "#코드 수정 필요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#예측 모델 적용 함수 정의 (WBS단계별로 정의)\n",
    "def execute_prediction(values, p3, p4, pa7_1, pb7_1):\n",
    "    p3.append(predictlv3(values))\n",
    "    p4.append(predictlv4(values))\n",
    "    pa7_1.append(predictlv7_1(values))\n",
    "    pb7_1.append(predictlv7_2(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#구간별(각행)에 대하여 예측 모델 적용.\n",
    "df = pd.DataFrame(columns=['I3', 'I4', 'I5', 'I7'])\n",
    "#df는 공종, 시설물, 부위, 작업자관리단위를 컬럼명으로 가지고 각각의 입력값의 번호를 행번호로 가지는 데이터프레임\n",
    "for i, cvalues in enumerate(input_data):\n",
    "    p3, p4, pa7_1, pb7_1 = [], [], [], [] #WBS 단계별 리스트 정의\n",
    "    execute_prediction(cvalues, p3, p4, pa7_1, pb7_1) #예측 모델 적용\n",
    "    row_data = {'I3': p3[0], 'I4': p4[0], 'I5': pa7_1[0], 'I7': pb7_1[0]}\n",
    "    df = df.append(row_data, ignore_index=True)\n",
    "#이후 데이터프레임에서 DB 정보와 비교하여 일치하는 내용을 추출하면 됨. 23.03.22 기준 웹서비스에서는\n",
    "#I5와 I7만 내용만 작성하도록 되어 있어 해당 열의 정보만 비교하면 될 것으로 판단됨.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
